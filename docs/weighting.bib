
@techreport{creedy_survey_2003,
	type = {Treasury {Working} {Paper} {Series}},
	title = {Survey {Reweighting} for {Tax} {Microsimulation} {Modelling}},
	url = {http://ideas.repec.org/p/nzt/nztwps/03-17.html},
	abstract = {This paper describes a range of ‘minimum distance’ methods used to compute new weights for large cross-sectional surveys used in microsimulation modelling. Extraneous information about a range of population variables is used for calibration purposes. An iterative solution procedure is described and numerical examples are given, involving comparisons among alternative distance functions. An application to the New Zealand Household Economic Survey (HES) is reported.},
	number = {03/17},
	institution = {New Zealand Treasury},
	author = {Creedy, John},
	month = sep,
	year = {2003},
	keywords = {Household, calibration;, survey, surveys;, weights},
}

@article{deville_calibration_1992,
	title = {Calibration {Estimators} in {Survey} {Sampling}},
	volume = {87},
	copyright = {Copyright © 1992 American Statistical Association},
	issn = {01621459},
	url = {http://www.jstor.org/stable/2290268},
	abstract = {This article investigates estimation of finite population totals in the presence of univariate or multivariate auxiliary information. Estimation is equivalent to attaching weights to the survey data. We focus attention on the several weighting systems that can be associated with a given amount of auxiliary information and derive a weighting system with the aid of a distance measure and a set of calibration equations. We briefly mention an application to the case in which the information consists of known marginal counts in a two- or multi-way table, known as generalized raking. The general regression estimator (GREG) was conceived with multivariate auxiliary information in mind. Ordinarily, this estimator is justified by a regression relationship between the study variable y and the auxiliary vector x. But we note that the GREG can be derived by a different route by focusing instead on the weights. The ordinary sampling weights of the kth observation is 1/π$_{\textrm{k}}$, where π$_{\textrm{k}}$ is the inclusion probability of k. We show that the weights implied by the GREG are as close as possible, according to a given distance measure, to the 1/π$_{\textrm{k}}$ while respecting side conditions called calibration equations. These state that the sample sum of the weighted auxiliary variable values must equal the known population total for that auxiliary variable. That is, the calibrated weights must give perfect estimates when applied to each auxiliary variables and the study variable means that the weights that perform well for the auxiliary variable also should perform well for the study variable. The GREG uses the auxiliary information efficiently, so the estimates are precise; however, the individual weights are not always without reproach. For example, negative weights can occur, and in some applications this does not make sense. It is natural to seek the root of the dissatisfaction in the underlying distance measure. Consequently, we allow alternative distance measures that satisfy only a set of minimal requirements. Each distance measure leads, via the calibration equations, to a specific weighting system and thereby to a new estimator. These estimators form a family of calibration estimators. We show that the GREG is a first approximation to all other members of the family; all are asymptotically equivalent to the GREG, and the variance estimator already known for the GREG is recommended for use in any other member of the family. Numerical features of the weights and ease of computation become more than anything else the bases for choosing between the estimators. The reasoning is applied to calibration on known marginals of a two-way frequency table. Our family of distance measures leads in this case to a family of generalized raking procedures, of which classical raking ratio is one.},
	language = {English},
	number = {418},
	journal = {Journal of the American Statistical Association},
	author = {Deville, Jean-Claude and Sarndal, Carl-Erik},
	year = {1992},
	pages = {pp. 376--382},
}

@misc{dwp_initial_2014,
	title = {Initial review of the {Family} {Resources} {Survey} weighting scheme - {GOV}.{UK}},
	url = {https://www.gov.uk/government/publications/initial-review-of-the-family-resources-survey-weighting-scheme},
	abstract = {A review of the Family Resources Survey weighting scheme.},
	urldate = {2017-10-23},
	author = {DWP},
	month = jun,
	year = {2014},
	file = {Snapshot:/home/graham_s/.mozilla/firefox/zy2vanb1.default/zotero/storage/GAGGKWUQ/initial-review-of-the-family-resources-survey-weighting-scheme.html:text/html},
}

@techreport{creedy_reweighting_2003,
	title = {Reweighting the {New} {Zealand} {Household} {Economic} {Survey} for {Tax} {Microsimulation} {Modelling}},
	url = {https://ideas.repec.org/p/nzt/nztwps/03-33.html},
	abstract = {This paper reports a reweighting exercise for the New Zealand Household Economic Survey, which is the basis of the Treasury's microsimulation model, TaxMod. Comparisons of benefit expenditures in a variety of demographic groups, along with population data, reveal that TaxMod estimates differ substantially from totals based on administrative data, when the weights provided by Statistics New Zealand are used. After describing the method used to compute new weights, the calibration requirements are reported. These relate to the age structure of the population and the number of beneficiaries for Unemployment Benefit, Domestic Purposes Benefit, Invalid's and Sickness Benefits and Family Support and Tax Credits. The revised weights and expenditure estimates are reported and the resulting distribution of income examined. The new weights are found to produce much improved expenditure estimates, without distorting the resulting income distribution. The effects of reweighting are demonstrated using a simple policy simulation.},
	language = {en},
	number = {03/33},
	urldate = {2018-02-07},
	institution = {New Zealand Treasury},
	author = {Creedy, John and Tuckwell, Ivan},
	month = dec,
	year = {2003},
	keywords = {microsimulation, minimum distance, Survey weights},
	file = {Fullext PDF:/home/graham_s/.mozilla/firefox/zy2vanb1.default/zotero/storage/AAGRSMX5/Creedy and Tuckwell - 2003 - Reweighting the New Zealand Household Economic Sur.pdf:application/pdf;Snapshot:/home/graham_s/.mozilla/firefox/zy2vanb1.default/zotero/storage/YLY5WNYI/03-33.html:text/html},
}

@misc{national_centre_for_research_methods_adjusting_nodate,
	title = {Adjusting for non-response by weighting},
	url = {http://www.restore.ac.uk/PEAS/nonresponse.php},
	urldate = {2018-10-18},
	author = {National Centre for Research Methods},
	file = {Adjusting for non-response by weighting:/home/graham_s/.mozilla/firefox/zy2vanb1.default/zotero/storage/EDRIDPD3/nonresponse.html:text/html},
}

@techreport{dsouza_stata_2010,
	title = {A {Stata} program for calibration weighting},
	url = {https://ideas.repec.org/p/boc/usug10/02.html},
	abstract = {Although survey data are sometimes weighted by their selection weights, it is often preferable to use auxiliary information available on the whole population to improve estimation. Calibration weighting (Deville and Sarndal, 1992, Journal of the American Statistical Association 87: 376-382) is one of the most common methods of doing this. This method adjusts the selection weights so that known population totals for the auxiliary variables are reproduced exactly, while ensuring that the calibrated weights are as close as possible to the original sampling weight. The simplest example of calibration is poststratification. This is the special case where the auxiliary variable is a single categorical variable. General calibration extends this to deal with more than one auxiliary variable and allows the user to include both categorical and numerical variables. A typical example might occur in a population survey, where the selection weights could be calibrated to ensure that the sample weighted by the calibration weights has exactly the same distribution as the population on variables such as age, sex, and region. Many packages have routines for calibration. SAS has the macro CALMAR; GenStat has the procedure SVCALIBRATE; and R has the function calibrate. However, no such routine is publicly available in Stata. I will introduce a user-written Stata program for calibration and will also discuss a simple extension to show how it can incorporate a nonresponse correction. I will also briefly discuss the program's strengths and limitations when compared to rival packages.},
	language = {en},
	number = {02},
	urldate = {2018-10-18},
	institution = {Stata Users Group},
	author = {D'Souza, John},
	month = sep,
	year = {2010},
	file = {Snapshot:/home/graham_s/.mozilla/firefox/zy2vanb1.default/zotero/storage/PB6Z6UT2/02.html:text/html},
}

@book{institut_national_de_statistique_brussel_generalised_2001,
	series = {Statistics {Belgium} working paper},
	title = {Generalised calibration at {Statistics} {Belgium} : {SPSS} module g-{CALIB}-{S} and current practices},
	shorttitle = {Generalised calibration at {Statistics} {Belgium}},
	language = {eng},
	publisher = {Bruxelles : Inst. National de Statistique},
	author = {Institut National de Statistique {\textless}Brüssel{\textgreater} and Vanderhoeft, Camille},
	year = {2001},
	file = {Generalised calibration at Statistics Belgium \: SPSS module g-CALIB-S and current practices - EconBiz:/home/graham_s/.mozilla/firefox/zy2vanb1.default/zotero/storage/M77NHUDY/10001732423.html:text/html},
}
